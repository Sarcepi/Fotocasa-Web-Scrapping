{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe77ebc-78de-4f9d-9860-fd42e9288c4d",
   "metadata": {},
   "source": [
    "# Fotocasa Web Scrapping\n",
    "Here i will show you how to extract apartment data from the website Fotocasa.\n",
    "The idea for getting the data its pretty simple, since Fotocasa its a dynamic website we need to interact with it in order to be able to retrive the information we want, that means we need to scroll down the page and get the html source code as the scroll down action its performd.Once we have the hmtl file its a matter of using the BeautifulSoup library to find the information within the bins and storing all that in a df.When we reach the end of the page the code will click the next page buttom and move foward , repeting the process once again.\n",
    "\n",
    "How are we going to interact with the web page? With the selenium web driver.\n",
    "\n",
    "Obviosly every web page has its owns particularitys so in this case we the need to account for the different setups that Fotocasa has thorght the web page.\n",
    "\n",
    "So the firts step as usal its to import the librarys we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "d2b53ca6-82c7-4795-8e0b-cb9251865564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4bed4-13eb-4532-9f86-bd04c777da13",
   "metadata": {},
   "source": [
    "### Importing and preparing the webdriver \n",
    "In this step its required to download the chrome web driver and indicate the path where the driver is.Finally we setup our webdriver with selenium.If you dont have it you can download it in here https://googlechromelabs.github.io/chrome-for-testing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "24c9bf26-35ad-41e7-a3b0-57e6877282da",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "chrome_driver_path = \"/Users/aa/Desktop/Proyectos/Rent_Price/chromedriver\"# Your path\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb378ac-35ba-46da-b885-95dc6c6a24f1",
   "metadata": {},
   "source": [
    "The next chunk of code its going to be perfect example of what selenium driver can do. First we indicate the web page we want to go to. Then we use the XPATH for the cookies button to locate that in a object to then click that path.Then we go into the rent aparments and finally in the search bar we introduce Madrid as the city want to look for aparments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "15f4fd0c-5757-450c-8cd3-e803b8b0e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.fotocasa.com/')# Select the web page ,in this case fotocasa.com\n",
    "driver.implicitly_wait(5)  \n",
    "#Click cookies button\n",
    "cookie_button = driver.find_element(By.XPATH, '//button[@aria-label=\"Aceptar y cerrar: Aceptar nuestro procesamiento de datos y cerrar\"]')\n",
    "cookie_button.click()\n",
    "#Select Rent apartments\n",
    "alquiler= driver.find_element(By.XPATH,'//div[@class=\"re-HomeSearchSelector-item re-HomeSearchSelector-item--rent\"]')\n",
    "alquiler.click()\n",
    "#Search for Madrid\n",
    "input_field = driver.find_element(By.XPATH, '//input[@class=\"sui-AtomInput-input sui-AtomInput-input-size-m\"]')\n",
    "input_field.send_keys(\"Madrid\")\n",
    "time.sleep(1)\n",
    "input_field.send_keys(Keys.ENTER)\n",
    "\n",
    "element = WebDriverWait(driver=driver, timeout=5).until(\n",
    "    EC.presence_of_element_located((By.XPATH, '//span[@class=\"re-CardPrice\"]'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c760eb92-acad-4fca-90f5-77b2e2d30775",
   "metadata": {},
   "source": [
    "### Function to extract the data\n",
    "Once we have done that its necessary to write the function that will be extracting the features from the aparments. In this case its call extract_data_all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "d067213c-8505-44de-bd3a-d7b2c1052ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_combined(soup):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    prices = []\n",
    "    square_m = []\n",
    "    rooms = []\n",
    "    bathrooms = []\n",
    "    zonas = []\n",
    "\n",
    "    def extract_from_icons(soup):\n",
    "        \"\"\"Extracts data from containers with icons.\"\"\"\n",
    "        square_meters = []\n",
    "        rooms = []\n",
    "        bathrooms = []\n",
    "\n",
    "        # Square meters\n",
    "        for sqm_container in soup.find_all(\n",
    "            'span', class_='re-CardFeaturesWithIcons-feature-icon re-CardFeaturesWithIcons-feature-icon--surface'\n",
    "        ):\n",
    "            sqm_text = sqm_container.get_text(strip=True)\n",
    "            numbers = re.findall(r'\\d+(?:[\\.,]\\d+)?', sqm_text)\n",
    "            square_meters.append(''.join(numbers) if numbers else None)\n",
    "\n",
    "        # Number of rooms\n",
    "        for room_container in soup.find_all(\n",
    "            'span', class_='re-CardFeaturesWithIcons-feature-icon re-CardFeaturesWithIcons-feature-icon--rooms'\n",
    "        ):\n",
    "            room_text = room_container.get_text(strip=True)\n",
    "            numbers = re.findall(r'\\d+', room_text)\n",
    "            rooms.append(''.join(numbers) if numbers else None)\n",
    "\n",
    "        # Number of bathrooms\n",
    "        for bathroom_container in soup.find_all(\n",
    "            'span', class_='re-CardFeaturesWithIcons-feature-icon re-CardFeaturesWithIcons-feature-icon--bathrooms'\n",
    "        ):\n",
    "            bathroom_text = bathroom_container.get_text(strip=True)\n",
    "            numbers = re.findall(r'\\d+', bathroom_text)\n",
    "            bathrooms.append(''.join(numbers) if numbers else None)\n",
    "\n",
    "        return square_meters, rooms, bathrooms\n",
    "\n",
    "    def extract_without_icons(soup):\n",
    "        \"\"\"Extracts data from containers without icons.\"\"\"\n",
    "        square_meters = []\n",
    "        rooms = []\n",
    "        bathrooms = []\n",
    "\n",
    "        feature_lists = soup.find_all('ul', class_='re-CardFeatures-wrapper')\n",
    "        for feature_list in feature_lists:\n",
    "            features = feature_list.find_all(\n",
    "                'li', class_='re-CardFeatures-item re-CardFeatures-feature'\n",
    "            )\n",
    "\n",
    "            # Extract features in sequence\n",
    "            number_of_rooms = (\n",
    "                features[0].get_text(strip=True) if len(features) > 0 else None\n",
    "            )\n",
    "            number_of_bathrooms = (\n",
    "                features[1].get_text(strip=True) if len(features) > 1 else None\n",
    "            )\n",
    "            square_meters_text = (\n",
    "                features[2].get_text(strip=True) if len(features) > 2 else None\n",
    "            )\n",
    "\n",
    "            # Extract numeric values\n",
    "            rooms.append(\n",
    "                ''.join(re.findall(r'\\d+', number_of_rooms)) if number_of_rooms else None\n",
    "            )\n",
    "            bathrooms.append(\n",
    "                ''.join(re.findall(r'\\d+', number_of_bathrooms)) if number_of_bathrooms else None\n",
    "            )\n",
    "            square_meters.append(\n",
    "                ''.join(re.findall(r'\\d+', square_meters_text)) if square_meters_text else None\n",
    "            )\n",
    "\n",
    "        return square_meters, rooms, bathrooms\n",
    "\n",
    "    # Extract prices\n",
    "    for price_container in soup.find_all('span', class_='re-CardPrice'):\n",
    "        price_text = price_container.get_text(strip=True)\n",
    "        numbers = re.findall(r'\\d+(?:[\\.,]\\d+)?', price_text)\n",
    "        prices.append(''.join(numbers) if numbers else None)\n",
    "\n",
    "    square_m, rooms, bathrooms = extract_from_icons(soup)\n",
    "\n",
    "    if not any(rooms) or not any(bathrooms):\n",
    "        square_m, rooms, bathrooms = extract_without_icons(soup)\n",
    "\n",
    "    # Extract neighborhood\n",
    "    for i in soup.find_all('span', class_='re-I18nPropertyTitle'):\n",
    "        word = i.get_text(strip=True)\n",
    "        if ',' in word:\n",
    "            parts = word.split(',')\n",
    "            zonas.append(parts[-1].strip())\n",
    "        else:\n",
    "            split_word = word.split()\n",
    "            zonas.append(split_word[-1])\n",
    "    #Padding list\n",
    "    max_length = max(len(prices), len(square_m), len(rooms), len(bathrooms), len(zonas))\n",
    "\n",
    "    prices.extend([None] * (max_length - len(prices)))\n",
    "    square_m.extend([None] * (max_length - len(square_m)))\n",
    "    rooms.extend([None] * (max_length - len(rooms)))\n",
    "    bathrooms.extend([None] * (max_length - len(bathrooms)))\n",
    "    zonas.extend([None] * (max_length - len(zonas)))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Price': prices,\n",
    "        'Square Meters': square_m,\n",
    "        'Rooms': rooms,\n",
    "        'Bathrooms': bathrooms,\n",
    "        'Zona': zonas\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459cf4e-8f1d-4178-9a17-e85bd6b66e55",
   "metadata": {},
   "source": [
    "### Code to scroll down and move to the next page while scrapping\n",
    "The next part its going to scroll down the web page and use the previuous function to extract the prices\n",
    "Now we are going to extract the prices for the first 30 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "9337e8f0-10a5-435c-b9fe-14d7b577c781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 prices: 31\n",
      "Page 2 prices: 30\n",
      "Page 3 prices: 30\n",
      "Page 4 prices: 30\n",
      "Page 5 prices: 30\n",
      "Page 6 prices: 30\n",
      "Page 7 prices: 30\n",
      "Page 8 prices: 30\n",
      "Page 9 prices: 30\n",
      "Page 10 prices: 30\n",
      "Page 11 prices: 30\n",
      "Page 12 prices: 30\n",
      "Page 13 prices: 30\n",
      "Page 14 prices: 30\n",
      "Page 15 prices: 30\n",
      "Page 16 prices: 30\n",
      "Page 17 prices: 30\n",
      "Page 18 prices: 30\n",
      "Page 19 prices: 30\n",
      "Page 20 prices: 30\n",
      "Page 21 prices: 30\n",
      "Page 22 prices: 30\n",
      "Page 23 prices: 30\n",
      "Page 24 prices: 30\n",
      "Page 25 prices: 30\n",
      "Page 26 prices: 30\n",
      "Page 27 prices: 30\n",
      "Page 28 prices: 30\n",
      "Page 29 prices: 30\n",
      "Page 30 prices: 30\n",
      "Error while processing page 31: Message: disconnected: not connected to DevTools\n",
      "  (failed to check if window was closed: disconnected: not connected to DevTools)\n",
      "  (Session info: chrome=131.0.6778.86)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010bf4ce82 chromedriver + 6696578\n",
      "1   chromedriver                        0x000000010bf44c9a chromedriver + 6663322\n",
      "2   chromedriver                        0x000000010b94ce3e chromedriver + 405054\n",
      "3   chromedriver                        0x000000010b933481 chromedriver + 300161\n",
      "4   chromedriver                        0x000000010b933374 chromedriver + 299892\n",
      "5   chromedriver                        0x000000010b94f702 chromedriver + 415490\n",
      "6   chromedriver                        0x000000010b9e4e0b chromedriver + 1027595\n",
      "7   chromedriver                        0x000000010b9c2613 chromedriver + 886291\n",
      "8   chromedriver                        0x000000010b98f950 chromedriver + 678224\n",
      "9   chromedriver                        0x000000010b99034e chromedriver + 680782\n",
      "10  chromedriver                        0x000000010bf1a770 chromedriver + 6489968\n",
      "11  chromedriver                        0x000000010bf1d219 chromedriver + 6500889\n",
      "12  chromedriver                        0x000000010bf1cd3b chromedriver + 6499643\n",
      "13  chromedriver                        0x000000010bf1d6a5 chromedriver + 6502053\n",
      "14  chromedriver                        0x000000010bf05524 chromedriver + 6403364\n",
      "15  chromedriver                        0x000000010bf1d98f chromedriver + 6502799\n",
      "16  chromedriver                        0x000000010bef6954 chromedriver + 6342996\n",
      "17  chromedriver                        0x000000010bf35378 chromedriver + 6599544\n",
      "18  chromedriver                        0x000000010bf35535 chromedriver + 6599989\n",
      "19  chromedriver                        0x000000010bf44868 chromedriver + 6662248\n",
      "20  libsystem_pthread.dylib             0x00007ff8149501d3 _pthread_start + 125\n",
      "21  libsystem_pthread.dylib             0x00007ff81494bbd3 thread_start + 15\n",
      "\n",
      "Total prices extracted: 0\n"
     ]
    }
   ],
   "source": [
    "all_prices = []\n",
    "scroll_pause_time = 0.5 \n",
    "df = pd.DataFrame(columns=['Price', 'Square Meters', 'Rooms', 'Bathrooms','Zona'])\n",
    "\n",
    "def close_ad_iframe():\n",
    "    try:\n",
    "        iframes = driver.find_elements(By.TAG_NAME, 'iframe')\n",
    "        for iframe in iframes:\n",
    "            driver.switch_to.frame(iframe)\n",
    "            driver.switch_to.default_content()\n",
    "    except Exception as e:\n",
    "        print(f\"Error handling ad iframe: {e}\")\n",
    "\n",
    "def click_next_button():\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '(//li[@class=\"sui-MoleculePagination-item\"])[last()]'))\n",
    "        )\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(next_button).click().perform()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'Next' button: {e}\")\n",
    "        return False\n",
    "#Select the number of pages you want to scrape\n",
    "a=soup.find_all('li',class_='sui-MoleculePagination-item')\n",
    "number_of_pages = int(a[-2].get_text()) \n",
    "for page_number in range(number_of_pages): \n",
    "    try:\n",
    "        \n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        for i in range(0, last_height, 500): \n",
    "            driver.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "        html_txt = driver.page_source\n",
    "        soup = BeautifulSoup(html_txt, 'html.parser')\n",
    "        \n",
    "        pp = extract_data_combined(soup)\n",
    "        df = pd.concat([df, pp], ignore_index=True)\n",
    "\n",
    "        print(f\"Page {page_number + 1} prices: {len(pp)}\")\n",
    "        \n",
    "        close_ad_iframe()\n",
    "        \n",
    "        if not click_next_button():\n",
    "            print(\"Failed to click 'Next' button. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(3)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing page {page_number + 1}: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"Total prices extracted: {len(all_prices)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4704f5e-639b-4728-996d-c964eb63bf4c",
   "metadata": {},
   "source": [
    "WARNING:for some reason i dont fully understand yet, you need to open the window where the scrapping its been done and wait till it gets to end of the fist page, otherwise i have experienced mistakes in the code so keep this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "13a9e083-ae6a-4659-8e94-e4bc00b52665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Square Meters</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Zona</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.300</td>\n",
       "      <td>208</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Hispanoamérica - Bernabéu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>985</td>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Cristóbal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>790</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Huertas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.395</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Butarque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.595</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>El Cañaveral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>2.500</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Castellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>2.250</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cuatro Caminos - Azca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>2.250</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ríos Rosas - Nuevos Ministerios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>2.500</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Recoletos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>4.000</td>\n",
       "      <td>207</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Goya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>901 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price Square Meters Rooms Bathrooms                             Zona\n",
       "0    4.300           208     3         2        Hispanoamérica - Bernabéu\n",
       "1      985            63     3         1                        Cristóbal\n",
       "2      790            28     1         1                          Huertas\n",
       "3    1.395            45     2         1                         Butarque\n",
       "4    1.595            90     1         2                     El Cañaveral\n",
       "..     ...           ...   ...       ...                              ...\n",
       "896  2.500            76     2         1                       Castellana\n",
       "897  2.250            42     1         1            Cuatro Caminos - Azca\n",
       "898  2.250            51     1         1  Ríos Rosas - Nuevos Ministerios\n",
       "899  2.500            60     1         1                        Recoletos\n",
       "900  4.000           207     4         3                             Goya\n",
       "\n",
       "[901 rows x 5 columns]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "e52361f0-b8c3-405c-81c5-8e5d0e3916f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('madrid_data.csv', index=False) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
